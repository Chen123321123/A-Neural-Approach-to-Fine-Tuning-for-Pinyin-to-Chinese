{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/royshi/Desktop/SFU/CMPT 413/Spring 2025/nlpclass-1251-g-GGBond/project/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pypinyin\n",
    "import re\n",
    "import pandas as pd\n",
    "from pypinyin import Style\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (Example: Standard clean data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: https://huggingface.co/datasets/swaption2009/20k-en-zh-translation-pinyin-hsk\n",
    "\n",
    "ds = load_dataset(\"swaption2009/20k-en-zh-translation-pinyin-hsk\")\n",
    "dataset = ds[\"train\"]\n",
    "\n",
    "contains_english = re.compile(r'[a-zA-Z]')\n",
    "\n",
    "def clean_punctuations(p):\n",
    "    \"\"\"\n",
    "    Remove common Chinese-style or western punctuation\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[。.,，！？!?:：；;\\\"'‘’“”()（）《》【】＇｀……\\-－／/、\\[\\]［］＂·—]\", \"\", p)\n",
    "\n",
    "def clean_spaces(text):\n",
    "    \"\"\"\n",
    "    Remove all spaces from Chinese text\n",
    "    \"\"\"\n",
    "    text = text.replace(\" \", \"\").replace(\"\\u00A0\", \"\").replace(\"　\", \"\")  # Remove regular and non-breaking spaces\n",
    "    return text\n",
    "\n",
    "def convert_fullwidth_to_normal(text):\n",
    "    \"\"\"\n",
    "    Convert full-width digits (０１２３４５６７８９) to normal digits (0123456789).\n",
    "    \"\"\"\n",
    "    return \"\".join(chr(ord(char) - 0xFEE0) if '０' <= char <= '９' else char for char in text)\n",
    "\n",
    "def chinese_to_pinyin(text):\n",
    "    return \" \".join(pypinyin.lazy_pinyin(text, style=Style.NORMAL))\n",
    "\n",
    "formatted_dataset, formatted_dataset_eval = [], []\n",
    "\n",
    "for i in range(2, dataset.num_rows, 5):\n",
    "    chinese = dataset[i][\"text\"][10:]\n",
    "    pinyin = dataset[i + 1][\"text\"][8:]\n",
    "\n",
    "    chinese = clean_punctuations(chinese)\n",
    "    chinese = clean_spaces(chinese)\n",
    "    chinese = convert_fullwidth_to_normal(chinese)\n",
    "\n",
    "    pinyin = chinese_to_pinyin(chinese)\n",
    "\n",
    "    if ((i+3) % 2000) == 0:\n",
    "        if len(chinese) < 60 and not contains_english.search(chinese) and pinyin not in [entry[\"Pinyin\"] for entry in formatted_dataset_eval]:\n",
    "            formatted_dataset_eval.append({\n",
    "                \"Pinyin\": pinyin,\n",
    "                \"Chinese\": chinese\n",
    "            })\n",
    "    else:\n",
    "        if len(chinese) < 60 and not contains_english.search(chinese) and chinese not in [entry[\"Chinese\"] for entry in formatted_dataset]:\n",
    "            formatted_dataset.append({\n",
    "                \"Chinese\": chinese,\n",
    "                \"Pinyin\": pinyin\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(formatted_dataset)\n",
    "df_eval = pd.DataFrame(formatted_dataset_eval)\n",
    "\n",
    "df.to_csv(\"train.csv\", index=False, encoding=\"utf-8\")\n",
    "df_eval.to_csv(\"eval.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Example: mT5-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"train.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "examples = []\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notna(row.get(\"Pinyin\")) and pd.notna(row.get(\"Chinese\")):\n",
    "        examples.append({\n",
    "            \"input\": row[\"Pinyin\"],\n",
    "            \"target\": row[\"Chinese\"]\n",
    "        })\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(examples))\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_subset, eval_subset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_dataset = Dataset.from_list([dataset[i] for i in train_subset.indices])\n",
    "eval_dataset = Dataset.from_list([dataset[i] for i in eval_subset.indices])\n",
    "del dataset\n",
    "\n",
    "model_name = \"google/mt5-base\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prefix = \"拼音转中文：\"\n",
    "    inputs = [prefix + text for text in examples[\"input\"]]\n",
    "    targets = examples[\"target\"] # Chinese\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        truncation=True,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "\n",
    "    label_ids = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "bleu = load(\"bleu\")\n",
    "chrf = load(\"chrf\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "def space_chars(text):\n",
    "    return \" \".join(list(text.strip()))\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    print(\"Pred shape:\", np.array(preds).shape)\n",
    "    print(\"Labels shape:\", np.array(labels).shape)\n",
    "\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "\n",
    "    # clip predictions to valid token ID range\n",
    "    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "\n",
    "    invalid = [(i, val) for i, row in enumerate(labels) for val in row if val < 0 or val >= tokenizer.vocab_size]\n",
    "    if invalid:\n",
    "        print(\"Invalid token IDs found:\", invalid[:5])  # print just a few for now\n",
    "        raise ValueError(\"Found token ids out of tokenizer vocab range.\")\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Normalize whitespace, remove special tokens, etc. if needed\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # Character-level accuracy\n",
    "    char_correct = 0\n",
    "    char_total = 0\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        char_total += len(label)\n",
    "        char_correct += sum(p == l for p, l in zip(pred, label))\n",
    "\n",
    "    char_accuracy = char_correct / char_total if char_total > 0 else 0.0\n",
    "\n",
    "    spaced_preds = [space_chars(pred) for pred in decoded_preds]\n",
    "    spaced_labels = [space_chars(label) for label in decoded_labels]\n",
    "\n",
    "    # BLEU (optional, use for logging or reference)\n",
    "    bleu_result = bleu.compute(predictions=spaced_preds, references=[[lbl] for lbl in spaced_labels])\n",
    "    bleu_score = bleu_result[\"bleu\"]\n",
    "\n",
    "    # chrf\n",
    "    chrf_score = chrf.compute(predictions=spaced_preds, references=[[lbl] for lbl in spaced_labels])[\"score\"]\n",
    "\n",
    "    # ROUGE (use spaced strings so it treats each char as a token)\n",
    "    rouge_result = rouge.compute(predictions=spaced_preds, references=spaced_labels, use_stemmer=False)\n",
    "    rouge1 = rouge_result[\"rouge1\"]\n",
    "    rouge2 = rouge_result[\"rouge2\"]\n",
    "    rougeL = rouge_result[\"rougeL\"]\n",
    "\n",
    "    return {\n",
    "        \"char_accuracy\": char_accuracy,\n",
    "        \"chrf\": chrf_score,\n",
    "        \"bleu\": bleu_score,\n",
    "        \"rouge1\": rouge1,\n",
    "        \"rouge2\": rouge2,\n",
    "        \"rougeL\": rougeL,\n",
    "    }\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5_pinyin_to_chinese\",  \n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,   \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,     \n",
    "    num_train_epochs=20,         \n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    "    metric_for_best_model=\"char_accuracy\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_steps=200,                    \n",
    "    logging_steps=100,         \n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    eval_steps=200,          \n",
    "    save_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model(\"./mt5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate (Example: Against standard clean data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./mt5-base\"\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "eval_csv_path = \"./eval.csv\"\n",
    "\n",
    "eval_data = pd.read_csv(eval_csv_path)\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "for sample in tqdm(eval_data.to_dict(orient=\"records\")):\n",
    "    input_text = \"拼音转中文：\" + sample[\"Pinyin\"]\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=\"longest\", truncation=True)[\"input_ids\"]\n",
    "    input_ids = torch.LongTensor(input_ids).view(1, -1).to(model.device)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=64,\n",
    "        num_beams=4,  \n",
    "        early_stopping=True,  \n",
    "        num_return_sequences=1,  \n",
    "    )\n",
    "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Pinyin: \" + sample[\"Pinyin\"] + \"\\nChinese: \" + pred + \"\\n\\n\")\n",
    "    preds.append(pred.strip())\n",
    "    labels.append(sample[\"Chinese\"].strip())\n",
    "\n",
    "def space_chars(text):\n",
    "    return \" \".join(list(text.strip()))\n",
    "\n",
    "char_correct = 0\n",
    "char_total = 0\n",
    "for pred, label in zip(preds, labels):\n",
    "    char_total += len(label)\n",
    "    char_correct += sum(p == l for p, l in zip(pred, label))\n",
    "char_accuracy = char_correct / char_total if char_total > 0 else 0.0\n",
    "\n",
    "spaced_preds = [space_chars(p) for p in preds]\n",
    "spaced_labels = [space_chars(l) for l in labels]\n",
    "\n",
    "bleu_result = bleu.compute(predictions=spaced_preds, references=[[l] for l in spaced_labels])\n",
    "chrf_result = chrf.compute(predictions=spaced_preds, references=[[l] for l in spaced_labels])\n",
    "rouge_result = rouge.compute(predictions=spaced_preds, references=spaced_labels, use_stemmer=False)\n",
    "\n",
    "results = {\n",
    "    \"char_accuracy\": char_accuracy,\n",
    "    \"bleu\": bleu_result[\"bleu\"],\n",
    "    \"chrf\": chrf_result[\"score\"],\n",
    "    \"rouge1\": rouge_result[\"rouge1\"],\n",
    "    \"rouge2\": rouge_result[\"rouge2\"],\n",
    "    \"rougeL\": rouge_result[\"rougeL\"],\n",
    "    \"predictions\": preds,\n",
    "    \"references\": labels\n",
    "}\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
